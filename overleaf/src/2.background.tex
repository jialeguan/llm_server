\section{Background}

\subsection{LLM}

Language modeling, as the fundamental function of language models (LMs), involves modeling the likelihood of
the word sequence and predicting the distribution of subsequent words. Over recent years, researchers have discovered
that scaling up language models not only enhances their
language modeling ability but also engenders emergent
capabilities for tackling more intricate tasks beyond conven-
tional NLP tasks [25]. These scaled-up language models are
referred to as large language models (LLMs).

The mainstream LLMs are designed based on the Transformer architecture [26]. Specifically, a typical Transformer architecture is composed of several stacked Transformer
blocks. Typically, a Transformer block consists of a Multi-Head Self-Attention (MHSA) block, a Feed Forward Net-
work (FFN), and a LayerNorm (LN) operation. For each
block, it receives the output features of the previous one
as the input, and passes the features through each sub-
module to obtain the output. Specially, before the first block,
a tokenizer is used to convert the original input sentence
into a sequence of tokens, and a following embedding layer
serves to convert the tokens into the input features. Then,
the additional position embeddings are added into the input
features to encode the sequential order of each input token.

The core concept of the Transformer architecture is the
self-attention mechanism, which is adopted in the MHSA
block. Specifically, denoted the input features as $X=[x_1,x_2,\dots,x_n]$, the MHSA block applies linear projection to
them and obtains a set of queries Q, keys K and values V as

\begin{equation*}
    Q_i=XW^{Q_i}, K_i=XW^{K_i}, V_i=XW^{V_i},
\end{equation*}

where $W^{Q_i}$, $W^{K_i}$ and $W^{V_i}$ are the projection matrices
corresponding to the $i$-th attention head. Then the self-
attention operation is applied to each tuple of $(Q_i,K_i,V_i)$ and get the feature of the $i$-th attention head $Z_i$ as:

\begin{equation*}
    Z_i = \text{softmax}\frac{Q_iK_i^T}{\sqrt{d_k}}V_i,
\end{equation*}

where $d_k$ is the dimension of the key vectors. Note that
the self-attention operation contains the matrix multipli-
cation operation, its computation complexity is quadratic
in the input length. Finally, the MHSA block concatenates
the features of all the attention heads and applies a linear
projection to them to form its output $Z$ as:

\begin{equation*}
    Z = \text{Concat}(Z_1,Z_2,\dots,Z_h)W^O,
\end{equation*}

where $h$ is the number of attention heads and $W^O$ is the projection. As can be seen, the
self-attention mechanism allows the model to identify the
importance of different input parts regardless of the dis-
tance, and thus can capture the long-range dependencies
and complex relationships in the input sentence.

Another important module in the Transformer block is
the FFN. Typically, FFN is placed after the MHSA block
and consists of two linear transformation layers with a non-
linear activation function. It receives the output features X
from the MHSA block and processes them as

\begin{equation*}
    \text{FFN}(X) = \text{ReLU}(XW_1+b_1)W_2+b_2,
\end{equation*}

where $W_1$, $W_2$, $b_1$ and $b_2$ are the learnable parameters

\subsection{LLM Inference}
The most commonly used models for tasks like text generation are decoder-only Language Models (LLMs), which typically employ an auto-regressive mechanism to generate output sequences token by token. In an auto-regressive framework, each token is generated by considering the tokens that have already been generated, along with the input sequence. However, as the length of the sequence increases, the computational cost of generating each token grows rapidly, making the process time-consuming.

To mitigate this issue, a crucial optimization technique known as the Key-Value (KV) Cache has been introduced. KV caching works by storing and reusing previously computed key-value pairs from the attention mechanism within the Multi-Head Self-Attention (MHSA) block. This allows the model to avoid recalculating certain components of the attention mechanism, thereby reducing the latency of generation. LLM inference systems heavily rely on KV caching to improve efficiency, enabling faster token generation without sacrificing the quality of the output.

The LLM inference process can be divided into two main stages based on how the KV cache is utilized: the Prefill stage and the Decode stage.

In Prefill stage, the LLM prefills the cache with the key-value pairs of the input tokens. Specifically, the LLM first processes the input tokens through the embedding layer and the position embedding layer to obtain the input features. Then, the LLM passes the input features through the MHSA block to generate the key-value pairs. Finally, the LLM stores the key-value pairs in the cache.

In Decode stage, the LLM generates the output tokens based on the input tokens and the prefilled cache. Specifically, the LLM first processes the input tokens through the embedding layer and the position embedding layer to obtain the input features. Then, the LLM passes the input features through the MHSA block. During the self-attention operation, the LLM retrieves the key-value pairs from the cache and uses them to calculate the attention scores. Finally, the LLM generates the output tokens based on the attention scores.

\definecolor{ngreen}{HTML}{D5E8D4}
\definecolor{nblue}{HTML}{DAE8FC}
\definecolor{npurple}{HTML}{E1D5E7}

\begin{figure*}[h]
    \centering
    \tikzset{
        basic/.style  = {draw, text width=2cm, align=center, font=\sffamily, rectangle},
        root/.style   = {basic, rounded corners=2pt, thin, align=center, fill=white,text width=8cm, rotate=90, font=\footnotesize},
        dnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=3.5cm, font=\footnotesize},
        dnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=2.5cm, font=\footnotesize},
        mnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=nblue, text width=3.5cm, font=\footnotesize},
        mnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=nblue, text width=2.5cm, font=\footnotesize},
        snode/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=3.5cm, font=\footnotesize},
        snode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=2.5cm, font=\footnotesize},
        tnode/.style = {basic, thin, align=left, fill=pink!60, text width=15em, align=center},
        xnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!20,text width=5cm,},
        wnode/.style = {basic, thin, align=left, fill=pink!10!blue!80!red!10, text width=6.5em},
        %edge from parent/.style = {draw=black, edge from parent fork right}
        %edge from parent/.style = {draw=black, edge from parent fork down}
    }
    %
    \begin{forest}
        for tree={
        if level=0{
        grow=east,
        growth parent anchor=east,
        parent anchor=south,
        child anchor=west,
        edge path={\noexpand\path[\forestoption{edge},->, >={latex}]
        (!u.parent anchor) -- +(5pt,0pt) |- (.child anchor)
        \forestoption{edge label};},
        }
        {
        grow=east,
        growth parent anchor=east,
        parent anchor=east,
        child anchor=west,
        edge path={\noexpand\path[\forestoption{edge},->, >={latex}]
        (!u.parent anchor) -- +(5pt,0pt) |- (.child anchor)
        \forestoption{edge label};},
        }
        }
        % l sep is used for arrow distance
        [Efficient Inference for Large Language Models, root
        [System-level Optimization (Sec.~\ref{sec:system-level-opt}), snode_1
        [Serving System \\ (Sec.~\ref{sec:serving_system}), snode
            [Distributed Systems, snode]
            [Scheduling, snode]
            [Batching, snode]
            [Memory Management, snode]
        ]
        [Inference Engine \\ (Sec.~\ref{sec:inference_engine}), snode
            %[Model Parallelism, snode]
            [Speculative Decoding, snode]
            [Offloading, snode]
            [Graph and Operator Optimization, snode]
        ]
        ]
        [Model-level Optimization (Sec.~\ref{sec:model-level-opt}), mnode_1
        [Model Compression \\ (Sec.~\ref{sec:model_compression}), mnode
            [Dynamic Inference, mnode]
            [Knowledge Distillation, mnode
                    [Black-box KD, mnode]
                    [White-box KD, mnode]
            ]
            [Structure Optimization, mnode
                    [Neural Architecture Search, mnode]
                    [Structure Factorization, mnode]
            ]
            [Sparsification, mnode
                    [Sparse Attention, mnode]
                    [Weight Pruning, mnode]
            ]
            [Quantization, mnode
                    [Quantization-aware Training, mnode]
                    [Post-Training Quantization, mnode]
            ]
        ]
        [Efficient Structure Design \\ (Sec.~\ref{sec:efficient_structure}), mnode
            [Transformer Alternate, mnode]
            [Efficient Attention Design, mnode
                    [Multi/Group-Query Attention, mnode]
                    [Low-Complexity Attention, mnode]
            ]
            [Efficient FFN Design, mnode]
        ]
        ]
        [Data-level Optimization (Sec.~\ref{sec:data-level-opt}), dnode_1
        [Output Organization \\ (Sec.~\ref{sec:output_compress}), dnode]
        [Input Compression \\ (Sec.~\ref{sec:input_compress}), dnode
            [Retrieval-Augmented Generation, dnode]
            [Soft Prompt-based Compression, dnode]
            [Prompt Summary, dnode]
            [Prompt Pruning, dnode]
        ]
        ]
        ]
    \end{forest}

    \caption{Taxonomy of efficient inference methods for Large Language Models.}
    \label{fig:framework}
\end{figure*}

\subsection{Private Cloud Compute}

The emergence of LLMs has introduced novel challenges in cloud computing, particularly in the realm of data privacy and security. Unlike traditional server-client architectures, which often maintain persistent user data and rely on policy-based privacy protections, LLM inference necessitates real-time access to unencrypted user requests and personal data, coupled with significantly higher computational demands. This paradigm shift exposes potential vulnerabilities in data handling and retention, as well as challenges in providing verifiable privacy guarantees and runtime transparency. To address these concerns, Apple has proposed Private Cloud Compute (PCC), a pioneering framework designed to extend device-level security into the cloud environment for AI processing. PCC establishes a set of stringent requirements that fundamentally reimagine cloud AI security: it mandates stateless computation on personal user data, ensuring that data is used solely for request fulfillment and is not retained post-processing; it demands technically enforceable guarantees that can be analyzed and constrained across all critical components; it eliminates privileged runtime access that could bypass privacy safeguards; it implements non-targetability to prevent attacks aimed at specific users; and it provides verifiable transparency, allowing security researchers to inspect and validate the system's integrity. These requirements collectively represent a significant departure from conventional cloud service security models, aiming to establish a new standard for secure and private AI processing in cloud environments.

\subsubsection{Taxonomy}

Disclosed privacy threats
